의사결정나무(decision tree)<br>

데이터 분류 및 회귀에 사용되는 지도학습 알고리즘<br>
 

장점

- 다른 알고리즘에 비해 결과값을 이해하기 쉬움

- 정확도가 높음


단점

- 과대적합되기 쉬움

---

데이터의 특징을 바탕으로 데이터를 연속으로 분리하다 보면 결국 하나의 정답으로 분류<br>

 

핵심: 의미있는 질문을 하는 것이 중요<br>

→ 데이터의 특징 속에서 분류에 큰 영향을 끼치는 특징을 상위 노드로 선택<br>

 

영향력 크기 비교 방법

- 엔트로피

- 지니계수

 #

**의사결정 트리 알고리즘과 정보 엔트로피의 관계**

정보를 획득한다=정답에 대한 불확실성이 줄어든다<br>

엔트로피(entropy) : 정보 이론(information theory)에서불확실성을 수치적으로 표현한 값<br>

정보 이득(information gain)=질문전의 엔트로피-질문 후의 엔트로피=불확실성이 줄어 든 정도<br>

Gain(T, X)=Entropy(T)-Entropy(T, X)

 #

**확률을 바탕으로 정보 엔트로피를 구하는 공식**

![2_1](https://github.com/ornni/ML_algorithm/blob/main/decision_tree/image/decision_tree_2-1.png?raw=true)

#

**한 가지 특징에 대한 엔트로피 계산**<br>

특징을 가지고 데이터를 분류할 수 있으므로 특징이 한 가지일 때의 엔트로피를 계산할 수 있다면 해당 특징을 확룔했을 때의 정보 이득도 알아낼 수 있음<br>

#

**특징에 대한 엔트로피를 계산하는 공식**<br>

하나의 특징으로 데이터를 분리했을 때의 엔트로피

![3_1](https://github.com/ornni/ML_algorithm/blob/main/decision_tree/image/decision_tree_3-1.png?raw=true)

X: 선택된 특징<br>

c: 선택된 특징에 의해 생성되는 하위 노드<br>

P(c): 선택된 특징에 의해 생성된 하위 노드에 데이터가 속할 확률<br>

E(c): 선택된 특징에 의해 생성된 하위 노드의 엔트로피<br>

- 잘 분리되면 엔트로피는 0에 가까워짐<br>

- 잘 분리되지 않으면 엔트로피는 1에 가까워짐<br>

 #

**지니 계수(Gini coefficient)**<br>

- 특징이 항상 이진 분류로 나뉠 때 사용됨<br>

- 지니 계수가 높을수록 순도가 높음=한 그룹에 모여있는데이터들의 속성이 많이 일치<br>

사이킷런의 의사결정 트리는 CART(classification and regression tree) 타입의 의사결정 트리, CART는 트리의 노드마다 특징을 이진 분류하는 특징이 있기에 사이킷런의 트리를 구성할 때 보통 지니 계수를 사용

 #

지니 계수를 통해 의사결정 트리의 노드를 결정하는 순서<br>

1. 특징으로 분리된 두 노드의 지니 계수를 구함<br>

2. 특징에 대한 지니 계수를 구함

#

**다중 분류**

다중 분류에서도 좋은 성능을 보임

---

**장점**

- 수학적인 지식이 없어도 결과를 해석하고 이해하기 쉽다

- 수치 데이터 및 범주 데이터에 모두 사용 가능하다


**단점**

- 과대적합의 위험이 높다
