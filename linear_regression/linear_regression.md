선형 회귀(linear regression)<br>

관찰된 데이터를 기반으로 하나의 함수를 구해서 관찰되지 않은 데이터의 값을 예측<br>

회귀 계수를 선형적으로 결합할 수 있는 모델

---

**회귀계수**

1차 함수의 기본 형식을 y=ax1+bx2라고 할 때 a, b를 회귀계수라고 함

#

**선형 결합(linear combination)**

선형 대수의 벡터의 합에서 나온 개념<br>

서로 다른 벡터를 더해 새로운 벡터를 얻는 과정<br>

단순히 더하는 것만이 아니라 가중치를 곱한 후 더하는 것도 포함

#

**더 나은 회귀 함수 선택하기(평균 제곱 오차)**

데이터포인트에서 함수의 선까지의 거리를 더하는 방법<br>

Square error=(실제값과예측값까지의 거리)2<br>

-> 평균 제곱 오차(MSE, mean squared error): f(θ)=1/n ∑(yi-θxi)2

#

**선형 회귀의 목적함수**

목적함수(objective function): 어떤 함수의 최댓값 또는 최솟값을 구하는 함수<br>

-> 평균 제곱 오차를 최소화해야 함!

#

**경사하강법(gradient descent)**

회귀모델을 구현할 때 최초 회귀 계수를 임의값으로 설정한 후, 반복적으로 경사하강법을 실행하여 최소의 평균 제곱 오차를 가지는 회귀 계수(θ)를 선택<br>

이때 학습률(learning rate, α)을 조정하여 θ값이 변경되는 정도를 의미함
