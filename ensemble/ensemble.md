앙상블

여러 개의 분류 모델을 조합해서 더 나은 성능을 내는 방법

---

**배깅(bagging)**

한 가지 분류 모델을 여러 개 만들어서 서로 다른 학습 데이터로 학습시킨 후(부트스트랩), 동일한 테스트 데이터에 대해 서로 다른 예측값들을 투표를 통해(어그리게이팅) 가장 높은 예측값으로 결정하는 앙상블 기법

 

- 부트스트랩

데이터를 조금 편향되게 샘플링하는 기법

 

- 어그리게이팅

여러 분류 모델이 에측한 값들을 조합해서 하나의 결론을 도출하는 과정

 

- 하드보팅

배깅에 포함된 분류모델들 중에서 최대 득표를 받은 예측값으로 결론 도출

 

- 소프트보팅

모든 분류값의 확률 리턴

각 분류값별 확률을 더해준 값을 점수로 사용해 최대점수를 가진 분류값을 결론으로 도출

 

- 랜덤 포레스트

여러 의사결정 트리를 배깅해서 예측을 실행하는 모델

각 노드에 주어진 데이터를 샘플링해서 일부 데이터를 제외한 채 최적의 특징을 찾아 트리를 분기

이 과정에서 또 한번의 모델의 편향을 증가시켜 과대적합의 위험을 감소

#

**부스팅**

여러 개의 분류기를 만들어서 투표를 통해 예측값을 결정(배깅과 공통점)

동일한 알고리즘의 분류기를 순차적으로 학습해서 여러 개의 분류기를 만들고, 테스트할 때 가중투표해서 예측값 결정(배깅과 차이점)

 

- 순차적 학습

순차적으로 학습 데이터를 보강하며, 동일한 알고리즘의 분류기를 여러 개 만드는 과정을 거침
